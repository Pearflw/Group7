---
title: "429final"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages('data.table')
```


```{r}
rm(list = ls())
library(data.table)
books <- fread('books.csv')
ratings <- fread('ratings.csv')
book_tags <- fread('book_tags.csv')
tags <- fread('tags.csv')
```


```{r}
#install.packages("readxl")
library("readxl")
library(gridExtra)
library("stringr")
books <- read_excel("all_books.xlsx")

#select columns
library(dplyr)
books <- books %>% select(id,book_id, work_id, books_count,authors, original_publication_year, title, language_code, average_rating,	work_ratings_count)

#clean language
table(books$language_code)
books$language_code[books$language_code == "en-CA"] <- "en"
books$language_code[books$language_code == "en-GB"] <- "en"
books$language_code[books$language_code == "en-US"] <- "en"
books$language_code[books$language_code == "eng"] <- "en"
```


```{r}
library(ggpubr)
theme_set(theme_pubr())

temp <- within(books, language_code <- factor(language_code,levels=names(sort(table(language_code), decreasing=FALSE))))
ggplot(temp,aes(x=language_code))+geom_bar()
plot1 <-ggplot(data.frame(temp), aes(x=language_code,fill=language_code)) +
  geom_bar()+
  labs(x = "language", title = "Including English") +
  theme_pubclean()+ 
  theme(legend.position = "none")+
  coord_flip()

books_wo_english <-books[!grepl("en", books$language_code),]
temp1 <- within(books_wo_english, language_code <- factor(language_code,levels=names(sort(table(language_code), decreasing=FALSE))))
plot2<-ggplot(data.frame(temp1), aes(x=language_code,fill=language_code)) +
  geom_bar()+
  labs(x = "language", title = "Excluding English") +
  theme_pubclean()+
  theme(legend.position = "none")+
  coord_flip()

grid.arrange(plot1, plot2, ncol=2)

# original_publication_year
hist(books$original_publication_year, col = "blue", main = "Publication year", xlab = " ")
# book count
hist(books$books_count, col = "blue", main = "Number of book editions", xlab = " ")
# Average Rating of books
hist(books$average_rating, col = "blue", main = "Average Rating", xlab = " ")

hist(books$work_ratings_count, col = "blue", main = "Average Rating", xlab = " ")

```

```{r}
#Genres
#first merge book_tags and tags
genres <- merge(book_tags,tags,by="tag_id")

# 2 approaches to genres
# use the genres provided by goodreads 

goodreads_genres <-(c("Art", "Biography", "Business", "Chick Lit", "Children's", "Christian", "Classics", "Comics", "Contemporary", "Cookbooks", "Crime", "Ebooks", "Fantasy", "Fiction", "Gay and Lesbian", "Graphic Novels", "Historical Fiction", "History", "Horror", "Humor and Comedy", "Manga", "Memoir", "Music", "Mystery", "Nonfiction", "Paranormal", "Philosophy", "Poetry", "Psychology", "Religion", "Romance", "Science", "Science Fiction", "Self Help", "Suspense", "Spirituality", "Sports", "Thriller", "Travel", "Young Adult"))

#str_to_lower(goodreads_genres)  %in% genres$tag_name

c <- goodreads_genres[str_to_lower(goodreads_genres) %in% genres$tag_name]

available_tags <- genres$tag_id[match(str_to_lower(c), genres$tag_name)]

genres <- genres %>% filter(genres$tag_id %in% available_tags)
#now the dataset contains a book with multiples tags but from the list above
temp4 <- within(genres, tag_name <- factor(tag_name,levels=names(sort(table(tag_name), decreasing=FALSE))))
ggplot(temp4,aes(x=tag_name))+geom_bar()
plot3 <- ggplot(data.frame(temp4), aes(x=tag_name,fill=tag_name)) +
  geom_bar()+
  labs(x = " ", title = "Allowing multiple genres") +
  theme_pubclean()+ 
  theme(legend.position = "none")+
  coord_flip()

# select the most popular one
require(data.table) 
genres_temp <- as.data.table(genres)
genres_temp <- genres_temp[genres_temp[, .I[which.max(count)], by=goodreads_book_id]$V1]

temp5 <- within(genres_temp, tag_name <- factor(tag_name,levels=names(sort(table(tag_name), decreasing=FALSE))))
ggplot(temp5,aes(x=tag_name))+geom_bar()
plot4 <- ggplot(data.frame(temp5), aes(x=tag_name,fill=tag_name)) +
  geom_bar()+
  labs(x = " ", title = "The most popular genre per") +
  theme_pubclean()+ 
  theme(legend.position = "none")+
  coord_flip()
par(mfrow=c(1,3))


grid.arrange(plot3, plot4, ncol=2)

#now use these last genres to match with the book dataset
final_genres <- genres_temp %>% select(goodreads_book_id, tag_id)
names(final_genres)[names(final_genres) == "goodreads_book_id"] <- "book_id"

#Final books dataset
all_books <- merge(books,final_genres,by="book_id")
```

```{r}
#convert the ratings dataset into rating matrix
library(tidyr)
dimension_indices <- list(user_id = sort(unique(ratings$user_id)), book_id = sort(unique(ratings$book_id)))
ratingmat <- spread(select(ratings, book_id, user_id, rating), book_id, rating) %>% select(-user_id) %>% as.matrix()
dimnames(ratingmat) <- dimension_indices
ratingmat[1:10, 1:10]
dim(ratingmat)
```

```{r}
#Apriori Algorithm

library(arules)
ratings <- fread('ratings.csv')
aaa=books
colnames(aaa)[2]='goodreads_book_ID'
colnames(aaa)[1]='book_id'
aaa$book=paste0(aaa$book_id,"_",aaa$title) 
books_with_id=select(aaa,book_id,book)
joined_ratings = left_join(x=ratings,y=books_with_id,by="book_id") 

#we choose the books that are rated >= 4 by user A as A's reading history with satisfaction
joined_ratings = joined_ratings[which(joined_ratings$rating>=4),]

#to get the sparse matrix
tr=split(joined_ratings$book_id,joined_ratings$user_id)
transactions = as(tr,"transactions")
summary(transactions)

#construct Apriori Algorithm
rules.Apriori = apriori(data=transactions, parameter = list(supp = 0.03, conf = 0.90, target = "rules",minlen=1,maxlen=10));
inspect(rules.Apriori)

#input some random user's reading history
basket = as(list(x=c(18,27,278,23,67)),"itemMatrix")
basket_subset = as.logical(is.subset(rules.Apriori@lhs,basket))
inspect(rules.Apriori[basket_subset])

#we choose the best five books to recommond by value of lift
rules.sorted<-sort(rules.Apriori[basket_subset],by='lift')
top5.rules<-head(rules.sorted, 5)
as(top5.rules,'data.frame')

```

```{r}
#Memory Based Recommendation 
library(arules)
ratings <- fread('ratings.csv')

aaa=books
colnames(aaa)[2]='goodreads_book_ID'
colnames(aaa)[1]='book_id'
aaa$book=paste0(aaa$book_id,"_",aaa$title) 
books_with_id=select(aaa,book_id,book)

#To eliminate running time, we sample the ratings dataset with the top 2000 books and the top 2,000 users
ratings <- fread('ratings.csv')
#ratings$book_id<-as.factor(ratings$book_id)
#ratings$user_id<-as.factor(ratings$user_id)

#Creating a subset with the top 2000 books and the top 2,000 users
x = head(data.frame(table(ratings$book_id) %>% sort(decreasing =T)), 2002)
colnames(x) = c("book_id", "Times_User")
#is(x$book_id[1])
x$book_id<-as.numeric(as.character(x$book_id))
the_books = merge(ratings, x, by="book_id")

y = head(data.frame(table(the_books$user_id) %>% sort(decreasing =T)), 2000) 
colnames(y) = c("user_id", "rates_Book")
y$user_id <- as.numeric(as.character(y$user_id))
ratings1 = merge(the_books, y, by="user_id")
ratings<-ratings1[,1:3]

ratings[, N := .N, .(user_id)]
ratings[, B := .N, .(book_id)]

dimension_indices <- list(user_id = sort(unique(ratings$user_id)), 
                          book_id = sort(unique(ratings$book_id)))
#unique(ratings$book_id)

summary(dimension_indices)


```

```{r}
library ( "recommenderlab" )

ratings$user_id <- as.numeric(as.factor(ratings$user_id)) 
ratings$book_id <- as.numeric(as.factor(ratings$book_id)) 

#construct sparse matrix
sparse_ratings <- sparseMatrix(i = ratings$user_id,j = ratings$book_id, x = ratings$rating, dims = c(length(unique(ratings$user_id)), length(unique(ratings$book_id))),dimnames = dimension_indices)

r <- new("realRatingMatrix", data = sparse_ratings) 
sparse_ratings[1:10,1:10]
colnames(sparse_ratings)[1:10]

#recommenderRegistry $ get_entry_names ( )    shows all the methods we can use

```

```{r}
#find the best nn for UBCF method. Cross-validation produces more robust results and error estimates.
ubcf<-data.frame()
for (i in c(10,20,30,40,50,100)){
  e <- evaluationScheme(r, method="cross-validation", k=10, given=-5)
  ubcf_add<-data.frame(Nearest_Neighbours = 0, test_RMSE=0) 
  Rec.ubcf <- Recommender(getData(e, "train"), "UBCF", param=list( nn=i))
  #making predictions on the test data set
  p_ubcf_test <- predict(Rec.ubcf, getData(e, "known"), type="ratings")
  # obtaining the error metrics for both approaches and comparing them
  test_RMSE_ubcf <-calcPredictionAccuracy(p_ubcf_test, getData(e, "unknown")) 
  ubcf_add$Nearest_Neighbours <-i
  ubcf_add$test_RMSE <- test_RMSE_ubcf[1]
  ubcf = rbind(ubcf,ubcf_add) 
}

#the result shows we use nn =20

#find the best k for IBCF method. Running this code takes about 15 mins.
ibcf<-data.frame()
for (i in c(10,20,30,50,100,200,500,1000)){
  e <- evaluationScheme(r, method="cross-validation", k=10, given=-5)
  ibcf_add<-data.frame(Nearest_Neighbours = 0, test_RMSE=0) 
  Rec.ibcf <- Recommender(getData(e, "train"), "IBCF", param=list( k=i))
  #making predictions on the test data set
  p_ibcf_test <- predict(Rec.ibcf, getData(e, "known"), type="ratings")
  # obtaining the error metrics for both approaches and comparing them
  test_RMSE_ibcf <-calcPredictionAccuracy(p_ibcf_test, getData(e, "unknown")) 
  ibcf_add$Nearest_Neighbours <-i
  ibcf_add$test_RMSE <- test_RMSE_ibcf[1]
  ibcf = rbind(ibcf,ibcf_add) 
}
#the result shows we use k =200

```

```{r}
#compare 6 methods :UBCF, IBCF, Popular, Random items,SVD, SVDF
#Running this code takes about 15-20 mins.
scheme <- evaluationScheme(r, method="cross-validation", k=10, given=-5, goodRating=4)
Algorithm <-list("UBCF_20"  = list(name="UBCF", param=list(nn = 20)),
                 "IBCF_200"  = list(name="IBCF", param=list(k  = 200)),
                 "popular"  = list(name="POPULAR", param=NULL),
                 "random items" = list(name="RANDOM", param=NULL),
                 "SVD" = list(name="SVD" ,param=NULL ),
                 "SVDF"= list(name="SVDF", param=NULL))
results <- evaluate(scheme, Algorithm , type = "ratings")
plot(results, ylim = c(0,2))
#getConfusionMatrix(results)[[1]]
```

```{r}
#you can use this code instead, takes only 5 mins
scheme <- evaluationScheme(r, method="split", train=0.9, given=-5, goodRating=4)
Algorithm <-list("UBCF_20"  = list(name="UBCF", param=list(nn = 20)),
                 "IBCF_200"  = list(name="IBCF", param=list(k  = 200)),
                 "popular"  = list(name="POPULAR", param=NULL),
                 "random items" = list(name="RANDOM", param=NULL),
                 "SVD" = list(name="SVD", param=NULL),
                 "SVDF"=list(name="SVDF", param=NULL))
results <- evaluate(scheme, Algorithm , type = "ratings")
plot(results, ylim = c(0,2))
```

```{r}
#select some user 7 to do recommendation

# Do prediction based on IBCF 

r_recom_IBCF <- Recommender ( r , method = "IBCF" , param=list(k  = 200))

# predict 5 books based on IBCF
pred_IBCF <- predict ( r_recom_IBCF , r [7] , type="ratings") 
top_ibcf <- getTopNLists(pred_IBCF, n=5)
top_IBCF_list <- as(top_ibcf,"list")

cat('The best 5 books to recommend (Based on IBCF)  are: \n')
for (i in 1:5) {
  b <- unique(books_with_id$book[books_with_id$book_id == top_IBCF_list[[1]][i]]) 
  artist <- unique(books$authors[books$id == top_IBCF_list[[1]][i]]) 
  print(paste(b,artist,sep = " - "))
}

cat('\n The best 5 books to recommend (Based on UBCF) are: \n')

# Do prediction based on UBCF 

r_recom_UBCF <- Recommender ( r , method = "UBCF" , param=list(nn = 20))
# predict 5 books based on UBCF
pred_UBCF <- predict ( r_recom_UBCF , r [7] , type="ratings") 
top.ubcf <- getTopNLists(pred_UBCF, n=5)
top_UBCF_list<-as ( top.ubcf , "list" )

for (i in 1:5) {
  b <- unique(books_with_id$book[books_with_id$book_id == top_UBCF_list[[1]][i]]) 
  artist <- unique(books$authors[books$id == top_UBCF_list[[1]][i]]) 
  print(paste(b,artist,sep = " - "))
}

```

```{r}
# predict 5 books based on SVD
# create the SVD model: (Recommender based on SVD approximation with column-mean imputation.)
r_recom_SVD <- Recommender ( r , method = "SVD" , param=NULL)
# do predictions
pred_SVD <- predict ( r_recom_SVD , r [7] , type="ratings") 
top_SVD <- getTopNLists(pred_SVD, n=5)
top_SVD_list <- as(top_SVD,"list")

cat('The best 5 books to recommend (Based on SVD)  are: \n')
for (i in 1:5) {
  b <- unique(books_with_id$book[books_with_id$book_id == top_SVD_list[[1]][i]]) 
  artist <- unique(books$authors[books$id == top_SVD_list[[1]][i]]) 
  print(paste(b,artist,sep = " - "))
}
```

```{r}
# predict 5 books based on FSVD
# create the FSVD model
# takes 15+ mins
r_recom_SVDF <- Recommender ( r , method = "SVDF" , param=list(k=10,gamma=0.015,lambda=0.001,min_epochs=50,max_epochs=200,min_improvement=1e-06,verbose = T))
# do predictions
pred_SVDF <- predict ( r_recom_SVDF , r [7] , type="ratings") 
top_SVDF <- getTopNLists(pred_SVDF, n=5)
top_SVDF_list <- as(top_SVDF,"list")

cat('The best 5 books to recommend (Based on SVD)  are: \n')
for (i in 1:5) {
  b <- unique(books_with_id$book[books_with_id$book_id == top_SVDF_list[[1]][i]]) 
  artist <- unique(books$authors[books$id == top_SVDF_list[[1]][i]]) 
  print(paste(b,artist,sep = " - "))
}
```

```{r}
# to see the two matrix U and V, run the following code, takes 15 mins
fsvd<- funkSVD(r, k = 10, gamma = 0.015, lambda = 0.001,
  min_improvement = 1e-06, min_epochs = 50, max_epochs = 200,
  verbose = T)
#fsvd$U
#fsvd$V
```

```{r}

```

```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```

```{r}

```


```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```


```{r}

```


```{r}

```


