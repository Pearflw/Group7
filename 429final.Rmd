---
title: "429final"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages('data.table')
```


```{r}
rm(list = ls())
library(data.table)
books <- fread('books.csv')
ratings <- fread('ratings.csv')
book_tags <- fread('book_tags.csv')
tags <- fread('tags.csv')
```


```{r}
#install.packages("readxl")
library("readxl")
library(gridExtra)
library("stringr")
books <- read_excel("all_books.xlsx")

#select columns
library(dplyr)
books <- books %>% select(id,book_id, work_id, books_count,authors, original_publication_year, title, language_code, average_rating,	work_ratings_count)

#clean language
table(books$language_code)
books$language_code[books$language_code == "en-CA"] <- "en"
books$language_code[books$language_code == "en-GB"] <- "en"
books$language_code[books$language_code == "en-US"] <- "en"
books$language_code[books$language_code == "eng"] <- "en"
```


```{r}
library(ggpubr)
theme_set(theme_pubr())

temp <- within(books, language_code <- factor(language_code,levels=names(sort(table(language_code), decreasing=FALSE))))
ggplot(temp,aes(x=language_code))+geom_bar()
plot1 <-ggplot(data.frame(temp), aes(x=language_code,fill=language_code)) +
  geom_bar()+
  labs(x = "language", title = "Including English") +
  theme_pubclean()+ 
  theme(legend.position = "none")+
  coord_flip()

books_wo_english <-books[!grepl("en", books$language_code),]
temp1 <- within(books_wo_english, language_code <- factor(language_code,levels=names(sort(table(language_code), decreasing=FALSE))))
plot2<-ggplot(data.frame(temp1), aes(x=language_code,fill=language_code)) +
  geom_bar()+
  labs(x = "language", title = "Excluding English") +
  theme_pubclean()+
  theme(legend.position = "none")+
  coord_flip()

grid.arrange(plot1, plot2, ncol=2)

# original_publication_year
hist(books$original_publication_year, col = "blue", main = "Publication year", xlab = " ")
# book count
hist(books$books_count, col = "blue", main = "Number of book editions", xlab = " ")
# Average Rating of books
hist(books$average_rating, col = "blue", main = "Average Rating", xlab = " ")

hist(books$work_ratings_count, col = "blue", main = "Average Rating", xlab = " ")

```

```{r}
#Genres
#first merge book_tags and tags
genres <- merge(book_tags,tags,by="tag_id")

# 2 approaches to genres
# use the genres provided by goodreads 

goodreads_genres <-(c("Art", "Biography", "Business", "Chick Lit", "Children's", "Christian", "Classics", "Comics", "Contemporary", "Cookbooks", "Crime", "Ebooks", "Fantasy", "Fiction", "Gay and Lesbian", "Graphic Novels", "Historical Fiction", "History", "Horror", "Humor and Comedy", "Manga", "Memoir", "Music", "Mystery", "Nonfiction", "Paranormal", "Philosophy", "Poetry", "Psychology", "Religion", "Romance", "Science", "Science Fiction", "Self Help", "Suspense", "Spirituality", "Sports", "Thriller", "Travel", "Young Adult"))

#str_to_lower(goodreads_genres)  %in% genres$tag_name

c <- goodreads_genres[str_to_lower(goodreads_genres) %in% genres$tag_name]

available_tags <- genres$tag_id[match(str_to_lower(c), genres$tag_name)]

genres <- genres %>% filter(genres$tag_id %in% available_tags)
#now the dataset contains a book with multiples tags but from the list above
temp4 <- within(genres, tag_name <- factor(tag_name,levels=names(sort(table(tag_name), decreasing=FALSE))))
ggplot(temp4,aes(x=tag_name))+geom_bar()
plot3 <- ggplot(data.frame(temp4), aes(x=tag_name,fill=tag_name)) +
  geom_bar()+
  labs(x = " ", title = "Allowing multiple genres") +
  theme_pubclean()+ 
  theme(legend.position = "none")+
  coord_flip()

# select the most popular one
require(data.table) 
genres_temp <- as.data.table(genres)
genres_temp <- genres_temp[genres_temp[, .I[which.max(count)], by=goodreads_book_id]$V1]

temp5 <- within(genres_temp, tag_name <- factor(tag_name,levels=names(sort(table(tag_name), decreasing=FALSE))))
ggplot(temp5,aes(x=tag_name))+geom_bar()
plot4 <- ggplot(data.frame(temp5), aes(x=tag_name,fill=tag_name)) +
  geom_bar()+
  labs(x = " ", title = "The most popular genre per") +
  theme_pubclean()+ 
  theme(legend.position = "none")+
  coord_flip()
par(mfrow=c(1,3))


grid.arrange(plot3, plot4, ncol=2)

#now use these last genres to match with the book dataset
final_genres <- genres_temp %>% select(goodreads_book_id, tag_id)
names(final_genres)[names(final_genres) == "goodreads_book_id"] <- "book_id"

#Final books dataset
all_books <- merge(books,final_genres,by="book_id")
```

```{r}
#convert the ratings dataset into rating matrix
library(tidyr)
dimension_indices <- list(user_id = sort(unique(ratings$user_id)), book_id = sort(unique(ratings$book_id)))
ratingmat <- spread(select(ratings, book_id, user_id, rating), book_id, rating) %>% select(-user_id) %>% as.matrix()
dimnames(ratingmat) <- dimension_indices
ratingmat[1:10, 1:10]
dim(ratingmat)
```

```{r}
#Content Based Recommendation
#reduce the dimensionaility of tags and creating the item profile
#get the main categories from tags for each book
main_tags_labels = c("art", "biography", "business", "chick Lit", "children's", "christian", "classics", "comics", "contemporary", "cookbooks", "crime", "ebooks", "fantasy", "fiction", "gay and lesbian", "graphic novels", "historical fiction", "history", "horror", "humor and comedy", "manga", "memoir", "music", "mystery", "nonfiction", "paranormal", "philosophy", "poetry", "psychology", "religion", "romance", "science", "science fiction", "self help", "suspense", "spirituality", "sports", "thriller", "travel", "young adult")
#map two tables together by tag_id
main_tags = merge(x=book_tags,y=tags,by="tag_id")
#combine tags for the same book and the original tag data of 30,000 rows is reduced to 10,000 rows
main_tags1 = main_tags[,.(tags = paste(tag_name,collapse=",")),.(goodreads_book_id)]
#generate a book-genre matrix for each book
for(j in main_tags_labels){
  set(main_tags1,j = j,value = grepl(x = main_tags1$tags,pattern = j)*1)
  print(j)
}
main_tags1[,tags:=NULL]
bookDF = merge(x= books,y=main_tags1,by="goodreads_book_id")
bookDF = bookDF[,-1,drop=FALSE]
bookDF = bookDF[,-2:-9,drop=FALSE]
bookDF = bookDF[,-3:-14,drop=FALSE]
#take the first 50k data in this task due to the excessive computation time
userDF <- ratings[1:50000]

#cluster books based on their genre using k-means, set k = 5
clusterBooks<-function(bookDF){
  #get rid of book ids and titles
  bookDF<-bookDF[,-(1:2)]
  bookCluster<-kmeans(bookDF,5)
  return(bookCluster)
}
#find all books with the associated ratings that selected user has watched
UserInfo<-function(dat,id){
  #Select all rows for the same user and keep the columns book_id & rating
  a<-subset(dat, user_id==id,select=c(book_id, rating))
  # allocate 0 to the cluster column
  cluster<-0
  activeUser <- data.frame(a[order(a$book_id),] ,cluster)
  return(activeUser)
}
#assign to each book the corresponding cluster number
UserBookCluster<-function(bookCluster, activeUser){
  #create temporary dataframe to match cluster assignments to book_ids
  df1<- data.frame(cbind(bookDF$book_id, clusterNum = bookCluster$cluster))
  names(df1)<-c("book_id", "cluster")
  #this matches the cluster number to the activeUser book id
  activeUser$cluster<-df1[match(activeUser$book_id, df1$book_id),2]
  return(activeUser)
}
#calculate for each cluster the average of the book ratings
MeanClusterRating<-function(bookCluster, activeUser){
  #aggregate() function is used along with the cluster memberships to determine variable means for each cluster in the original metric
  like<-aggregate(activeUser$rating, by=list(cluster=activeUser$cluster), mean)
  #if the max mean rating is below three it gives out the dummy value zero
  if(max(like$x)<3){
    like<-as.vector(0)
    #else it gives out the cluster number of the max mean value
  } else{
    like<-as.vector(t(max(subset(like, x>=3, select=cluster))))
  }
  return(like)
}
#if there is no cluster with a rating of 3 or above, select at random 100 books
GoodBooks<-function(like, bookCluster, bookDF){
  #a temporary dataframe is created to get a list of all books and their associated clusters
  df1<- data.frame(cbind(bookDF$book_id, clusterNum = bookCluster$cluster))
  names(df1)<-c("book_id", "cluster")
  #if like has the value zero it selects randomly 100 books
  if(like==0){
    recommend<-bookDf[sample.int(n = dim(bookDF)[1], size = 100), 1]
  }
  #else it selects all books from the winning max mean cluster
  else{
    recommend<-as.vector(t(subset(df1, cluster==like, select=book_id)))
  }
  return(recommend)
}
#finalise the final recommendation list
RecommendedBooks<-function(bookDF, userDF, user_id){
  #recall all functions
  bookCluster<-clusterBooks(bookDF)
  activeUser<-UserInfo(userDF, user_id)
  activeUser<-UserBookCluster(bookCluster, activeUser)
  like<-MeanClusterRating(bookCluster, activeUser)
  recommend<-GoodBooks(like, bookCluster, bookDF)
  # only select not yet watched books
  recommend<-recommend[-activeUser$book_id]
  # add book title
  title<-bookDF[match(recommend,bookDF$book_id),2]
  recommend<-data.frame(recommend,title)
  return(recommend)
}
#make suggestion
#set the specific user id and the number of suggestions as inputs
suggestBooks<-function(bookDF, userDF, user_id, no_books){
  #get suggestions
  suggestions = RecommendedBooks(bookDF, userDF, user_id)
  #select stated number of selections
  suggestions = suggestions[1:no_books,]
  writeLines("You may also like these movies:")
  #print suggestions
  write.table(suggestions[2], row.names = FALSE, col.names = FALSE)
}
#try with an example: suggest 10 books to user whose ID is 1
suggestBooks(bookDF, userDF, 1, 10)
```

```{r}
#Apriori Algorithm

library(arules)
ratings <- fread('ratings.csv')
aaa=books
colnames(aaa)[2]='goodreads_book_ID'
colnames(aaa)[1]='book_id'
aaa$book=paste0(aaa$book_id,"_",aaa$title) 
books_with_id=select(aaa,book_id,book)
joined_ratings = left_join(x=ratings,y=books_with_id,by="book_id") 

#we choose the books that are rated >= 4 by user A as A's reading history with satisfaction
joined_ratings = joined_ratings[which(joined_ratings$rating>=4),]

#to get the sparse matrix
tr=split(joined_ratings$book_id,joined_ratings$user_id)
transactions = as(tr,"transactions")
summary(transactions)

#construct Apriori Algorithm
rules.Apriori = apriori(data=transactions, parameter = list(supp = 0.03, conf = 0.90, target = "rules",minlen=1,maxlen=10));
inspect(rules.Apriori)

#input some random user's reading history
basket = as(list(x=c(18,27,278,23,67)),"itemMatrix")
basket_subset = as.logical(is.subset(rules.Apriori@lhs,basket))
inspect(rules.Apriori[basket_subset])

#we choose the best five books to recommond by value of lift
rules.sorted<-sort(rules.Apriori[basket_subset],by='lift')
top5.rules<-head(rules.sorted, 5)
as(top5.rules,'data.frame')

```

```{r}
#Memory Based Recommendation 
library(arules)
ratings <- fread('ratings.csv')

aaa=books
colnames(aaa)[2]='goodreads_book_ID'
colnames(aaa)[1]='book_id'
aaa$book=paste0(aaa$book_id,"_",aaa$title) 
books_with_id=select(aaa,book_id,book)

#To eliminate running time, we sample the ratings dataset with the top 2000 books and the top 2,000 users
ratings <- fread('ratings.csv')
#ratings$book_id<-as.factor(ratings$book_id)
#ratings$user_id<-as.factor(ratings$user_id)

#Creating a subset with the top 2000 books and the top 2,000 users
x = head(data.frame(table(ratings$book_id) %>% sort(decreasing =T)), 2002)
colnames(x) = c("book_id", "Times_User")
#is(x$book_id[1])
x$book_id<-as.numeric(as.character(x$book_id))
the_books = merge(ratings, x, by="book_id")

y = head(data.frame(table(the_books$user_id) %>% sort(decreasing =T)), 2000) 
colnames(y) = c("user_id", "rates_Book")
y$user_id <- as.numeric(as.character(y$user_id))
ratings1 = merge(the_books, y, by="user_id")
ratings<-ratings1[,1:3]

ratings[, N := .N, .(user_id)]
ratings[, B := .N, .(book_id)]

dimension_indices <- list(user_id = sort(unique(ratings$user_id)), 
                          book_id = sort(unique(ratings$book_id)))
#unique(ratings$book_id)

summary(dimension_indices)


```

```{r}
library ( "recommenderlab" )

ratings$user_id <- as.numeric(as.factor(ratings$user_id)) 
ratings$book_id <- as.numeric(as.factor(ratings$book_id)) 

#construct sparse matrix
sparse_ratings <- sparseMatrix(i = ratings$user_id,j = ratings$book_id, x = ratings$rating, dims = c(length(unique(ratings$user_id)), length(unique(ratings$book_id))),dimnames = dimension_indices)

r <- new("realRatingMatrix", data = sparse_ratings) 
sparse_ratings[1:10,1:10]
colnames(sparse_ratings)[1:10]

#recommenderRegistry $ get_entry_names ( )    shows all the methods we can use

```

```{r}
#find the best nn for UBCF method. Cross-validation produces more robust results and error estimates.
ubcf<-data.frame()
for (i in c(10,20,30,40,50,100)){
  e <- evaluationScheme(r, method="cross-validation", k=10, given=-5)
  ubcf_add<-data.frame(Nearest_Neighbours = 0, test_RMSE=0) 
  Rec.ubcf <- Recommender(getData(e, "train"), "UBCF", param=list( nn=i))
  #making predictions on the test data set
  p_ubcf_test <- predict(Rec.ubcf, getData(e, "known"), type="ratings")
  # obtaining the error metrics for both approaches and comparing them
  test_RMSE_ubcf <-calcPredictionAccuracy(p_ubcf_test, getData(e, "unknown")) 
  ubcf_add$Nearest_Neighbours <-i
  ubcf_add$test_RMSE <- test_RMSE_ubcf[1]
  ubcf = rbind(ubcf,ubcf_add) 
}

#the result shows we use nn =20

#find the best k for IBCF method. Running this code takes about 15 mins.
ibcf<-data.frame()
for (i in c(10,20,30,50,100,200,500,1000)){
  e <- evaluationScheme(r, method="cross-validation", k=10, given=-5)
  ibcf_add<-data.frame(Nearest_Neighbours = 0, test_RMSE=0) 
  Rec.ibcf <- Recommender(getData(e, "train"), "IBCF", param=list( k=i))
  #making predictions on the test data set
  p_ibcf_test <- predict(Rec.ibcf, getData(e, "known"), type="ratings")
  # obtaining the error metrics for both approaches and comparing them
  test_RMSE_ibcf <-calcPredictionAccuracy(p_ibcf_test, getData(e, "unknown")) 
  ibcf_add$Nearest_Neighbours <-i
  ibcf_add$test_RMSE <- test_RMSE_ibcf[1]
  ibcf = rbind(ibcf,ibcf_add) 
}
#the result shows we use k =200

```

```{r}
#compare 6 methods :UBCF, IBCF, Popular, Random items,SVD, SVDF
#Running this code takes about 15-20 mins.
scheme <- evaluationScheme(r, method="cross-validation", k=10, given=-5, goodRating=4)
Algorithm <-list("UBCF_20"  = list(name="UBCF", param=list(nn = 20)),
                 "IBCF_200"  = list(name="IBCF", param=list(k  = 200)),
                 "popular"  = list(name="POPULAR", param=NULL),
                 "random items" = list(name="RANDOM", param=NULL),
                 "SVD" = list(name="SVD" ,param=NULL ),
                 "SVDF"= list(name="SVDF", param=NULL))
results <- evaluate(scheme, Algorithm , type = "ratings")
plot(results, ylim = c(0,2))
#getConfusionMatrix(results)[[1]]
```

```{r}
#you can use this code instead, takes only 5 mins
scheme <- evaluationScheme(r, method="split", train=0.9, given=-5, goodRating=4)
Algorithm <-list("UBCF_20"  = list(name="UBCF", param=list(nn = 20)),
                 "IBCF_200"  = list(name="IBCF", param=list(k  = 200)),
                 "popular"  = list(name="POPULAR", param=NULL),
                 "random items" = list(name="RANDOM", param=NULL),
                 "SVD" = list(name="SVD", param=NULL),
                 "SVDF"=list(name="SVDF", param=NULL))
results <- evaluate(scheme, Algorithm , type = "ratings")
plot(results, ylim = c(0,2))
```

```{r}
#select some user 7 to do recommendation

# Do prediction based on IBCF 

r_recom_IBCF <- Recommender ( r , method = "IBCF" , param=list(k  = 200))

# predict 5 books based on IBCF
pred_IBCF <- predict ( r_recom_IBCF , r [7] , type="ratings") 
top_ibcf <- getTopNLists(pred_IBCF, n=5)
top_IBCF_list <- as(top_ibcf,"list")

cat('The best 5 books to recommend (Based on IBCF)  are: \n')
for (i in 1:5) {
  b <- unique(books_with_id$book[books_with_id$book_id == top_IBCF_list[[1]][i]]) 
  artist <- unique(books$authors[books$id == top_IBCF_list[[1]][i]]) 
  print(paste(b,artist,sep = " - "))
}

cat('\n The best 5 books to recommend (Based on UBCF) are: \n')

# Do prediction based on UBCF 

r_recom_UBCF <- Recommender ( r , method = "UBCF" , param=list(nn = 20))
# predict 5 books based on UBCF
pred_UBCF <- predict ( r_recom_UBCF , r [7] , type="ratings") 
top.ubcf <- getTopNLists(pred_UBCF, n=5)
top_UBCF_list<-as ( top.ubcf , "list" )

for (i in 1:5) {
  b <- unique(books_with_id$book[books_with_id$book_id == top_UBCF_list[[1]][i]]) 
  artist <- unique(books$authors[books$id == top_UBCF_list[[1]][i]]) 
  print(paste(b,artist,sep = " - "))
}

```

```{r}
# predict 5 books based on SVD
# create the SVD model: (Recommender based on SVD approximation with column-mean imputation.)
r_recom_SVD <- Recommender ( r , method = "SVD" , param=NULL)
# do predictions
pred_SVD <- predict ( r_recom_SVD , r [7] , type="ratings") 
top_SVD <- getTopNLists(pred_SVD, n=5)
top_SVD_list <- as(top_SVD,"list")

cat('The best 5 books to recommend (Based on SVD)  are: \n')
for (i in 1:5) {
  b <- unique(books_with_id$book[books_with_id$book_id == top_SVD_list[[1]][i]]) 
  artist <- unique(books$authors[books$id == top_SVD_list[[1]][i]]) 
  print(paste(b,artist,sep = " - "))
}
```

```{r}
# predict 5 books based on FSVD
# create the FSVD model
# takes 15+ mins
r_recom_SVDF <- Recommender ( r , method = "SVDF" , param=list(k=10,gamma=0.015,lambda=0.001,min_epochs=50,max_epochs=200,min_improvement=1e-06,verbose = T))
# do predictions
pred_SVDF <- predict ( r_recom_SVDF , r [7] , type="ratings") 
top_SVDF <- getTopNLists(pred_SVDF, n=5)
top_SVDF_list <- as(top_SVDF,"list")

cat('The best 5 books to recommend (Based on SVD)  are: \n')
for (i in 1:5) {
  b <- unique(books_with_id$book[books_with_id$book_id == top_SVDF_list[[1]][i]]) 
  artist <- unique(books$authors[books$id == top_SVDF_list[[1]][i]]) 
  print(paste(b,artist,sep = " - "))
}
```

```{r}
# to see the two matrix U and V, run the following code, takes 15 mins
fsvd<- funkSVD(r, k = 10, gamma = 0.015, lambda = 0.001,
  min_improvement = 1e-06, min_epochs = 50, max_epochs = 200,
  verbose = T)
#fsvd$U
#fsvd$V
```

```{r}

```

```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```

```{r}

```


```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```


```{r}

```


```{r}

```


